{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hate_speech_classifier.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/avinsit123/Tweetopedia_Collab_Notebooks/blob/master/hate_speech_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "v7ddx-MYuUvC",
        "colab_type": "code",
        "outputId": "abd015c9-5f1d-492b-fa07-79a331ce2a88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1020
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install torch "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/92/419c6959c0c427909994f019c039bf963354fe50153fa3a41094782b3a43/torch-1.0.0-cp27-cp27mu-manylinux1_x86_64.whl (591.8MB)\n",
            "\u001b[K    100% |████████████████████████████████| 591.8MB 25kB/s \n",
            "tcmalloc: large alloc 1073750016 bytes == 0x558db538e000 @  0x7f6aa6dc42a4 0x558d53890f18 0x558d53984a85 0x558d538a44ca 0x558d538a9232 0x558d538a1d0a 0x558d538a95fe 0x558d538a1d0a 0x558d538a95fe 0x558d538a1d0a 0x558d538a95fe 0x558d538a1d0a 0x558d538a9c38 0x558d538a1d0a 0x558d538a95fe 0x558d538a1d0a 0x558d538a95fe 0x558d538a9232 0x558d538a9232 0x558d538a1d0a 0x558d538a9c38 0x558d538a9232 0x558d538a1d0a 0x558d538a9c38 0x558d538a1d0a 0x558d538a9c38 0x558d538a1d0a 0x558d538a95fe 0x558d538a1d0a 0x558d538a1629 0x558d538d261f\n",
            "\u001b[?25hInstalling collected packages: torch\n",
            "Successfully installed torch-1.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "P12ReKmesT_s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "import matplotlib as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dcarVpALsUAC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Import data "
      ]
    },
    {
      "metadata": {
        "id": "0FgB-rNXsUAG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "https://github.com/t-davidson/hate-speech-and-offensive-language/tree/master/data\n",
        "\n",
        "\"The data are stored as a CSV and as a pickled pandas dataframe (Python 2.7). Each data file contains 5 columns:\n",
        "\n",
        "- count = number of CrowdFlower users who coded each tweet (min is 3, sometimes more users coded a tweet when judgments were determined to be unreliable by CF).\n",
        "- hate_speech = number of CF users who judged the tweet to be hate speech.\n",
        "- offensive_language = number of CF users who judged the tweet to be offensive.\n",
        "- neither = number of CF users who judged the tweet to be neither offensive nor non-offensive.\n",
        "- class = class label for majority of CF users. 0 - hate speech 1 - offensive language 2 - neither\""
      ]
    },
    {
      "metadata": {
        "id": "hGDv3oMRyyOn",
        "colab_type": "code",
        "outputId": "a4d7edad-ce36-4909-8bab-2fbb05a3bb20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        }
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/t-davidson/hate-speech-and-offensive-language.git\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'hate-speech-and-offensive-language'...\n",
            "remote: Enumerating objects: 161, done.\u001b[K\n",
            "Receiving objects:   0% (1/161)   \rReceiving objects:   1% (2/161)   \rReceiving objects:   2% (4/161)   \rReceiving objects:   3% (5/161)   \rReceiving objects:   4% (7/161)   \rReceiving objects:   5% (9/161)   \rReceiving objects:   6% (10/161)   \rReceiving objects:   7% (12/161)   \rReceiving objects:   8% (13/161)   \rReceiving objects:   9% (15/161)   \rReceiving objects:  10% (17/161)   \rReceiving objects:  11% (18/161)   \rReceiving objects:  12% (20/161)   \rReceiving objects:  13% (21/161)   \rReceiving objects:  14% (23/161)   \rReceiving objects:  15% (25/161)   \rReceiving objects:  16% (26/161)   \rReceiving objects:  17% (28/161)   \rReceiving objects:  18% (29/161)   \rReceiving objects:  19% (31/161)   \rReceiving objects:  20% (33/161)   \rReceiving objects:  21% (34/161)   \rReceiving objects:  22% (36/161)   \rReceiving objects:  23% (38/161)   \rReceiving objects:  24% (39/161)   \rReceiving objects:  25% (41/161)   \rReceiving objects:  26% (42/161)   \rReceiving objects:  27% (44/161)   \rReceiving objects:  28% (46/161)   \rReceiving objects:  29% (47/161)   \rReceiving objects:  30% (49/161)   \rReceiving objects:  31% (50/161)   \rReceiving objects:  32% (52/161)   \rReceiving objects:  33% (54/161)   \rReceiving objects:  34% (55/161)   \rReceiving objects:  35% (57/161)   \rReceiving objects:  36% (58/161)   \rReceiving objects:  37% (60/161)   \rReceiving objects:  38% (62/161)   \rReceiving objects:  39% (63/161)   \rReceiving objects:  40% (65/161)   \rReceiving objects:  41% (67/161)   \rremote: Total 161 (delta 0), reused 0 (delta 0), pack-reused 161\u001b[K\n",
            "Receiving objects:  42% (68/161)   \rReceiving objects:  43% (70/161)   \rReceiving objects:  44% (71/161)   \rReceiving objects:  45% (73/161)   \rReceiving objects:  46% (75/161)   \rReceiving objects:  47% (76/161)   \rReceiving objects:  48% (78/161)   \rReceiving objects:  49% (79/161)   \rReceiving objects:  50% (81/161)   \rReceiving objects:  51% (83/161)   \rReceiving objects:  52% (84/161)   \rReceiving objects:  53% (86/161)   \rReceiving objects:  54% (87/161)   \rReceiving objects:  55% (89/161)   \rReceiving objects:  56% (91/161)   \rReceiving objects:  57% (92/161)   \rReceiving objects:  58% (94/161)   \rReceiving objects:  59% (95/161)   \rReceiving objects:  60% (97/161)   \rReceiving objects:  61% (99/161)   \rReceiving objects:  62% (100/161)   \rReceiving objects:  63% (102/161)   \rReceiving objects:  64% (104/161)   \rReceiving objects:  65% (105/161)   \rReceiving objects:  66% (107/161)   \rReceiving objects:  67% (108/161)   \rReceiving objects:  68% (110/161)   \rReceiving objects:  69% (112/161)   \rReceiving objects:  70% (113/161)   \rReceiving objects:  71% (115/161)   \rReceiving objects:  72% (116/161)   \rReceiving objects:  73% (118/161)   \rReceiving objects:  74% (120/161)   \rReceiving objects:  75% (121/161)   \rReceiving objects:  76% (123/161)   \rReceiving objects:  77% (124/161)   \rReceiving objects:  78% (126/161)   \rReceiving objects:  79% (128/161)   \rReceiving objects:  80% (129/161)   \rReceiving objects:  81% (131/161)   \rReceiving objects:  82% (133/161)   \rReceiving objects:  83% (134/161)   \rReceiving objects:  84% (136/161)   \rReceiving objects:  85% (137/161)   \rReceiving objects:  86% (139/161)   \rReceiving objects:  87% (141/161)   \rReceiving objects:  88% (142/161)   \rReceiving objects:  89% (144/161)   \rReceiving objects:  90% (145/161)   \rReceiving objects:  91% (147/161)   \rReceiving objects:  92% (149/161)   \rReceiving objects:  93% (150/161)   \rReceiving objects:  94% (152/161)   \rReceiving objects:  95% (153/161)   \rReceiving objects:  96% (155/161)   \rReceiving objects:  97% (157/161)   \rReceiving objects:  98% (158/161)   \rReceiving objects:  99% (160/161)   \rReceiving objects: 100% (161/161)   \rReceiving objects: 100% (161/161), 3.59 MiB | 24.67 MiB/s, done.\n",
            "Resolving deltas:   0% (0/67)   \rResolving deltas:   2% (2/67)   \rResolving deltas:  10% (7/67)   \rResolving deltas:  13% (9/67)   \rResolving deltas:  37% (25/67)   \rResolving deltas:  41% (28/67)   \rResolving deltas:  44% (30/67)   \rResolving deltas:  61% (41/67)   \rResolving deltas:  65% (44/67)   \rResolving deltas:  83% (56/67)   \rResolving deltas:  85% (57/67)   \rResolving deltas:  86% (58/67)   \rResolving deltas:  88% (59/67)   \rResolving deltas:  89% (60/67)   \rResolving deltas:  91% (61/67)   \rResolving deltas:  95% (64/67)   \rResolving deltas:  98% (66/67)   \rResolving deltas: 100% (67/67)   \rResolving deltas: 100% (67/67), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "B8jJGWIRsUAJ",
        "colab_type": "code",
        "outputId": "2130934d-2c08-4499-dce7-e62f96dcc71c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        }
      },
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"hate-speech-and-offensive-language/data/labeled_data.csv\")\n",
        "print(df.shape)\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(24783, 7)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>count</th>\n",
              "      <th>hate_speech</th>\n",
              "      <th>offensive_language</th>\n",
              "      <th>neither</th>\n",
              "      <th>class</th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  count  hate_speech  offensive_language  neither  class  \\\n",
              "0           0      3            0                   0        3      2   \n",
              "1           1      3            0                   3        0      1   \n",
              "2           2      3            0                   3        0      1   \n",
              "3           3      3            0                   2        1      1   \n",
              "4           4      6            0                   6        0      1   \n",
              "\n",
              "                                               tweet  \n",
              "0  !!! RT @mayasolovely: As a woman you shouldn't...  \n",
              "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n",
              "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  \n",
              "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
              "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "PjE8vY_JsUAU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Process data"
      ]
    },
    {
      "metadata": {
        "id": "6XSyMgVksUAX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Tokenize and encode"
      ]
    },
    {
      "metadata": {
        "id": "LF4aOqtxsUAa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Initialize the tokenizer\n",
        "tk = TweetTokenizer()\n",
        "\n",
        "# Define special tokens\n",
        "SOS = 'SOS_TOKEN'\n",
        "EOS = 'EOS_TOKEN'\n",
        "PAD = 'PAD_TOKEN'\n",
        "UNK = '<unknown>'\n",
        "\n",
        "vocab = set()\n",
        "\n",
        "# Begin mapping dictionaries for token-->word and word-->token\n",
        "word_to_idx = {PAD:0, SOS:1, EOS:2, UNK:3}\n",
        "idx_to_word = {0:PAD, 1:SOS, 2:EOS, 3:UNK}\n",
        "\n",
        "# Build word_to_idx and idx_to_word dictionaries\n",
        "for msg in df['tweet'].values:\n",
        "    for word in tk.tokenize(msg):\n",
        "        if word not in vocab:\n",
        "            vocab.add(word)\n",
        "            word_to_idx[word] = len(word_to_idx)\n",
        "            idx_to_word[len(idx_to_word)] = word"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dwRqV0EasUAj",
        "colab_type": "code",
        "outputId": "d4875a26-6e5b-4a9e-b971-549afe11e96f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 546
        }
      },
      "cell_type": "code",
      "source": [
        "def sentence_to_idx(sentence_str):\n",
        "    \"\"\"Transform sentence strings into index arrays\"\"\"\n",
        "    sentence_as_idx = [1] # Begin with SOS token\n",
        "    \n",
        "    for word in tk.tokenize(sentence_str):\n",
        "        if word_to_idx.get(word):\n",
        "            sentence_as_idx.append(word_to_idx[word])\n",
        "        else:\n",
        "            sentence_as_idx.append(word_to_idx[UNK])\n",
        "            \n",
        "    sentence_as_idx.append(2) # End with EOS token\n",
        "    return sentence_as_idx\n",
        "\n",
        "df['msg_idx'] = df['tweet'].apply(sentence_to_idx)\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>count</th>\n",
              "      <th>hate_speech</th>\n",
              "      <th>offensive_language</th>\n",
              "      <th>neither</th>\n",
              "      <th>class</th>\n",
              "      <th>tweet</th>\n",
              "      <th>msg_idx</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
              "      <td>[1, 4, 4, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
              "      <td>[1, 4, 4, 4, 5, 30, 7, 31, 32, 33, 29, 34, 35,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
              "      <td>[1, 4, 4, 4, 5, 44, 45, 4, 4, 4, 5, 46, 47, 7,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
              "      <td>[1, 4, 4, 4, 5, 61, 7, 62, 53, 63, 64, 9, 65, 2]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
              "      <td>[1, 4, 4, 4, 5, 66, 7, 67, 60, 11, 68, 14, 69,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  count  hate_speech  offensive_language  neither  class  \\\n",
              "0           0      3            0                   0        3      2   \n",
              "1           1      3            0                   3        0      1   \n",
              "2           2      3            0                   3        0      1   \n",
              "3           3      3            0                   2        1      1   \n",
              "4           4      6            0                   6        0      1   \n",
              "\n",
              "                                               tweet  \\\n",
              "0  !!! RT @mayasolovely: As a woman you shouldn't...   \n",
              "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...   \n",
              "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...   \n",
              "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...   \n",
              "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...   \n",
              "\n",
              "                                             msg_idx  \n",
              "0  [1, 4, 4, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...  \n",
              "1  [1, 4, 4, 4, 5, 30, 7, 31, 32, 33, 29, 34, 35,...  \n",
              "2  [1, 4, 4, 4, 5, 44, 45, 4, 4, 4, 5, 46, 47, 7,...  \n",
              "3   [1, 4, 4, 4, 5, 61, 7, 62, 53, 63, 64, 9, 65, 2]  \n",
              "4  [1, 4, 4, 4, 5, 66, 7, 67, 60, 11, 68, 14, 69,...  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "B3BMnyXSsUAv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load pretrained embeddings"
      ]
    },
    {
      "metadata": {
        "id": "qGUqlZF5YUUm",
        "colab_type": "code",
        "outputId": "ea47719c-f006-48e1-b863-d5e39e27695f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 949
        }
      },
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove*.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-01-01 12:26:50--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2019-01-01 12:26:50--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M   102MB/s    in 8.4s    \n",
            "\n",
            "2019-01-01 12:26:59 (98.3 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "scrolled": false,
        "id": "8D34sAeCsUAz",
        "colab_type": "code",
        "outputId": "e0e2bb03-c04b-4602-ee85-b50771f2720b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 94
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "glove_file = \"glove.6B.200d.txt\"\n",
        "EMBEDDING_DIM = 200\n",
        "\n",
        "embeddings_dict = {}\n",
        "f = open(glove_file)\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word, coefs = values[0], torch.FloatTensor(np.asarray(values[1:], dtype='float32'))\n",
        "    if word in vocab:\n",
        "        embeddings_dict[word] = coefs\n",
        "    elif word in ['eos','sos']:\n",
        "        embeddings_dict[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print(\"DONE: Prepared GloVe embeddings for {} words.\".format(len(embeddings_dict)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DONE: Prepared GloVe embeddings for 12959 words.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_sTIAt17sUA8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Use GloVe embeddings for SOS and EOS tokens,\n",
        "# according to the keys we used during encoding:\n",
        "embeddings_dict[SOS] = embeddings_dict['sos']\n",
        "embeddings_dict[EOS] = embeddings_dict['eos']\n",
        "\n",
        "# Use zeros for UNK and padding\n",
        "#(the padding embedding doesn't matter, will be ignored)\n",
        "embeddings_dict[UNK] = torch.zeros(EMBEDDING_DIM)\n",
        "embeddings_dict[PAD] = torch.zeros(EMBEDDING_DIM)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B6xtEEw5sUBH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Prepare target columns: `is_hate_speech` and `is_offensive_language`"
      ]
    },
    {
      "metadata": {
        "id": "uwPs0xo7sUBK",
        "colab_type": "code",
        "outputId": "4e0266b5-9886-4167-dc1e-1b128e82266c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 693
        }
      },
      "cell_type": "code",
      "source": [
        "df['is_hate_speech'] = df['class'] == 0\n",
        "df['is_offensive_language'] = df['class'] == 1\n",
        "\n",
        "print(\"Classes:\")\n",
        "print(df['class'].value_counts())\n",
        "\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classes:\n",
            "1    19190\n",
            "2     4163\n",
            "0     1430\n",
            "Name: class, dtype: int64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>count</th>\n",
              "      <th>hate_speech</th>\n",
              "      <th>offensive_language</th>\n",
              "      <th>neither</th>\n",
              "      <th>class</th>\n",
              "      <th>tweet</th>\n",
              "      <th>msg_idx</th>\n",
              "      <th>is_hate_speech</th>\n",
              "      <th>is_offensive_language</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
              "      <td>[1, 4, 4, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
              "      <td>[1, 4, 4, 4, 5, 30, 7, 31, 32, 33, 29, 34, 35,...</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
              "      <td>[1, 4, 4, 4, 5, 44, 45, 4, 4, 4, 5, 46, 47, 7,...</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
              "      <td>[1, 4, 4, 4, 5, 61, 7, 62, 53, 63, 64, 9, 65, 2]</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
              "      <td>[1, 4, 4, 4, 5, 66, 7, 67, 60, 11, 68, 14, 69,...</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  count  hate_speech  offensive_language  neither  class  \\\n",
              "0           0      3            0                   0        3      2   \n",
              "1           1      3            0                   3        0      1   \n",
              "2           2      3            0                   3        0      1   \n",
              "3           3      3            0                   2        1      1   \n",
              "4           4      6            0                   6        0      1   \n",
              "\n",
              "                                               tweet  \\\n",
              "0  !!! RT @mayasolovely: As a woman you shouldn't...   \n",
              "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...   \n",
              "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...   \n",
              "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...   \n",
              "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...   \n",
              "\n",
              "                                             msg_idx  is_hate_speech  \\\n",
              "0  [1, 4, 4, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...           False   \n",
              "1  [1, 4, 4, 4, 5, 30, 7, 31, 32, 33, 29, 34, 35,...           False   \n",
              "2  [1, 4, 4, 4, 5, 44, 45, 4, 4, 4, 5, 46, 47, 7,...           False   \n",
              "3   [1, 4, 4, 4, 5, 61, 7, 62, 53, 63, 64, 9, 65, 2]           False   \n",
              "4  [1, 4, 4, 4, 5, 66, 7, 67, 60, 11, 68, 14, 69,...           False   \n",
              "\n",
              "   is_offensive_language  \n",
              "0                  False  \n",
              "1                   True  \n",
              "2                   True  \n",
              "3                   True  \n",
              "4                   True  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "28W14zhPsUBS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Train/test/val split"
      ]
    },
    {
      "metadata": {
        "id": "q0Opd9ISsUBV",
        "colab_type": "code",
        "outputId": "fd0255b2-f52e-476a-a968-bacefe9b36af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        }
      },
      "cell_type": "code",
      "source": [
        "def get_train_test_val_splits(data, val_pct=.15, test_pct=.15):\n",
        "    \"\"\"\n",
        "    data: np.array shape: (n observations, features)\n",
        "    returns: np.array of shuffled, randomly assigned observations\n",
        "        for training, validation, and test datasets\n",
        "    \n",
        "    Ex: train_data, val_data, test_data = get_train_test_val_splits(data)\n",
        "    \"\"\"\n",
        "\n",
        "    n_observations = data.shape[0]\n",
        "    all_idx = np.arange(n_observations)\n",
        "    np.random.shuffle(all_idx)\n",
        "\n",
        "    train_pct = 1 - val_pct - test_pct\n",
        "\n",
        "    train_idx = all_idx[:int(np.floor(n_observations*train_pct))]\n",
        "    val_idx = all_idx[int(np.floor(n_observations*train_pct)):-int(np.floor(n_observations*test_pct))]\n",
        "    test_idx = all_idx[-int(np.floor(n_observations*test_pct)):]\n",
        "\n",
        "    train_data = np.array(data)[train_idx] \n",
        "    val_data = np.array(data)[val_idx] \n",
        "    test_data = np.array(data)[test_idx] \n",
        "    \n",
        "    return train_data, val_data, test_data\n",
        "\n",
        "data = np.asarray(df)\n",
        "train_data, val_data, test_data = get_train_test_val_splits(data)\n",
        "print(\"\"\"\n",
        "Training set: {}\n",
        "Validation set: {}\n",
        "Test set: {}\"\"\".format(train_data.shape, val_data.shape, test_data.shape))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training set: (17348, 10)\n",
            "Validation set: (3718, 10)\n",
            "Test set: (3717, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8lnWa3TdsUBc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Prepare training data"
      ]
    },
    {
      "metadata": {
        "id": "yIarUWon4TaW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let us have a look at the train data\n",
        " and its columns"
      ]
    },
    {
      "metadata": {
        "id": "urCdmHLI4aNN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "np.sum(train_data[:,8] == True,axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gwCeH-lfsUBe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Get glove embeddings for each word in training set, and use UNK embedding (torch.zeros(200)) if not found"
      ]
    },
    {
      "metadata": {
        "id": "YhcRXbl1sUBf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 10\n",
        "MAX_LENGTH = 30\n",
        "\n",
        "def dataset_into_batch_idx(data, batch_size, shuffle=True, max_batches_to_return=None):\n",
        "    \"\"\"\n",
        "    Return indices of the entire dataset arranged into batches.\n",
        "     \n",
        "    Inputs:\n",
        "    data: np.array of data, shape (n_observations, features)\n",
        "    batch_size: int\n",
        "    max_batches_to_return: (optional) Number of batches to return\n",
        "    \n",
        "    Returns:\n",
        "    np.array of indices. Dim 0: batches, Dim 1: indices within a batch.\n",
        "    \"\"\"\n",
        "    \n",
        "    \n",
        "    # Make sure there is enough data for at least 1 batch\n",
        "    if data.shape[0] < batch_size:\n",
        "        print(\"Not enough data for one batch.\")\n",
        "        return None\n",
        "    \n",
        "    idx = np.arange(data.shape[0])\n",
        "    \n",
        "    # Shuffle indices\n",
        "    if shuffle:\n",
        "        np.random.shuffle(idx)\n",
        "    \n",
        "    # Get rid of of any rows if there will be a remainder when divided data size by batch size\n",
        "    remainder = idx.shape[0] % batch_size\n",
        "    if remainder > 0:\n",
        "        idx = idx[:-remainder]\n",
        "    \n",
        "    # Rehape to ((-1, batch_size))\n",
        "    idx = idx.reshape((-1, batch_size))\n",
        "    \n",
        "    # Keep only enough rows for batch_size x max_batches_to_return\n",
        "    if max_batches_to_return:\n",
        "        idx = idx[:max_batches_to_return]    \n",
        "    \n",
        "    return idx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SE1SZIuWsUBi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Example:\n",
        "train_idx = dataset_into_batch_idx(train_data, BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K7TvrLp_sUBl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def prepare_batch(data, batch_idx, batch_size=BATCH_SIZE,\n",
        "                  max_seq_length=MAX_LENGTH,\n",
        "                  target='is_hate_speech'):\n",
        "\n",
        "    \"\"\"\n",
        "    Prepare source and target tensors for ONE batch\n",
        "    \n",
        "    Inputs:\n",
        "    data:\n",
        "    batch_idx: array of indices for a batch\n",
        "    batch_size\n",
        "    target: 'is_hate_speech' (default) or 'is_offensive_language'\n",
        "    \n",
        "\n",
        "    Returns:\n",
        "        tuple:\n",
        "        Variable tensor of embedded source sequence\n",
        "            (batch size, max seq length in batch,\n",
        "             embedding dim),\n",
        "        Variable tensor of targets\n",
        "            (batch size),\n",
        "        Tensor of original sequence lenths before\n",
        "            padding (batch size)\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    batch = data[batch_idx]\n",
        "\n",
        "    batch_source = batch[:,7] # 6 is index of the encoded words\n",
        "    \n",
        "    # Get length of each sequence in the batch (or MAX_LENGTH if it's greater than that)\n",
        "    batch_seq_lengths = torch.LongTensor([min(len(seq),max_seq_length) for seq in batch_source])\n",
        "\n",
        "    # Get max sequence length in batch, or MAX_LENGTH -- whichever is smaller\n",
        "    max_seq_length_in_batch = min(max([len(seq) for seq in batch_source]),\n",
        "                                  max_seq_length)\n",
        "\n",
        "    embedded_seqs = []\n",
        "    for seq in batch_source: # For each sequence in batch\n",
        "        seq_embedded = []\n",
        "        seq = seq[:max_seq_length_in_batch] # If sequence is longer than max length, cut if off\n",
        "        for word_idx in seq: # Get the GloVe embedding for each word in the sequence\n",
        "            if idx_to_word[word_idx] in embeddings_dict:\n",
        "                word_embedding = embeddings_dict[idx_to_word[word_idx]]\n",
        "            else: # If no GloVe embedding for the word, use the UNK embedding (zeros)\n",
        "                word_embedding = embeddings_dict[UNK]\n",
        "            seq_embedded.append(word_embedding)\n",
        "        for padding in range(max_seq_length_in_batch - len(seq)):\n",
        "            seq_embedded.append(embeddings_dict[PAD])\n",
        "        embedded_seqs.append(torch.stack(seq_embedded)) # Stack embedded sequence into a tensor \n",
        "\n",
        "    batch_source_embedded = Variable(torch.stack(embedded_seqs)) # Stack embadded sequences in the batch into a tensor\n",
        "\n",
        "    target_idx = 8 if target == 'is_hate_speech' else 9\n",
        "    batch_target = torch.FloatTensor(batch[:,target_idx].astype(int))\n",
        "    \n",
        "    # Sort by original (unpadded) sequence length, descending\n",
        "    _, sort_indices = torch.sort(batch_seq_lengths, dim=0, descending=True) \n",
        "    batch_source_embedded = batch_source_embedded[sort_indices,:,:]\n",
        "    batch_target = batch_target[sort_indices]\n",
        "    batch_seq_lengths = batch_seq_lengths[sort_indices]\n",
        "\n",
        "    return batch_source_embedded, Variable(batch_target), batch_seq_lengths"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gWfo9-aDsUBo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Example:\n",
        "batch_idx = train_idx[0]\n",
        "\n",
        "batch_source_embedded, batch_target, batch_seq_lengths = prepare_batch(train_data, batch_idx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-nIrJjDLsUBr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Models"
      ]
    },
    {
      "metadata": {
        "id": "mpngxjsdsUBt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "\n",
        "class LSTMClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    Bidirectional LSTM classifier\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hidden_dim, batch_size, embedding_size, num_layers=1, dropout=0, verbose=False):\n",
        "        super(LSTMClassifier, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.verbose = verbose\n",
        "        self.inner_hidden_dim = 200\n",
        "        \n",
        "        self.num_directions = 2 # Bidirectional\n",
        "\n",
        "        self.lstm = nn.LSTM(embedding_size, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.fc1 = nn.Linear(hidden_dim*2, self.inner_hidden_dim)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.fc2 = nn.Linear(self.inner_hidden_dim, 1)\n",
        "        self.sig = nn.Sigmoid()\n",
        "\n",
        "    def init_hidden(self):\n",
        "        # Tuple of two tensors: (h_0, c_0)\n",
        "        return (torch.zeros(self.num_layers*self.num_directions, self.batch_size, self.hidden_dim),\n",
        "                torch.zeros(self.num_layers*self.num_directions, self.batch_size, self.hidden_dim))\n",
        "\n",
        "    def forward(self, input, unpadded_lengths):\n",
        "\n",
        "        if self.verbose:\n",
        "            print('Input:',input.size())\n",
        "        \n",
        "        packed = pack_padded_sequence(input, unpadded_lengths, batch_first=True)\n",
        "        if self.verbose:\n",
        "            print(\"After packing:\",packed.data.size())\n",
        "        \n",
        "        packed_output, (hidden, cell) = self.lstm(packed, self.hidden)\n",
        "        \n",
        "        if self.verbose:\n",
        "            print('Packed output after LSTM:',packed_output.data.size())\n",
        "        \n",
        "        # Get last hidden state for each sequence\n",
        "        # For each batch, concat the last hidden state from the forward LSTM\n",
        "        # with the last hidden state from the reverse LSTM.\n",
        "        # (aka get the hidden state from the last _timestep_)\n",
        "        \n",
        "        def unzip(t):\n",
        "            \"\"\"\n",
        "            Returns a tensor where dimension 0 is the layer. So with 3 layers, shape[0] = 3.\n",
        "            Along each layer, the forward and backward hidden states are concatenated.\n",
        "            \"\"\"\n",
        "            return torch.cat([t[0:t.size(0):2], t[1:t.size(0):2]], 2)\n",
        "        \n",
        "        # Get the forward and backward hidden states for the top layer\n",
        "        out = unzip(hidden)[-1]\n",
        "        \n",
        "        out = self.dropout1(out)\n",
        "        \n",
        "        if self.verbose:\n",
        "            print('Top hidden layer from LSTM:', out.size())\n",
        "            \n",
        "        out = self.relu1(self.fc1(out))\n",
        "        out = self.dropout1(out)\n",
        "        out = self.sig(self.fc2(out))\n",
        "        out = out.squeeze(1)\n",
        "        \n",
        "        if self.verbose:\n",
        "            print('Output:', out.size())\n",
        "            \n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ojWI99tmsUBx",
        "colab_type": "code",
        "outputId": "e79dc565-f13a-41e6-c2b6-793220eeda18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        }
      },
      "cell_type": "code",
      "source": [
        "# Example:\n",
        "model = LSTMClassifier(hidden_dim=300, batch_size=BATCH_SIZE, embedding_size=EMBEDDING_DIM,\n",
        "                       num_layers=3, dropout=0, verbose=True)\n",
        "\n",
        "# Example batch:\n",
        "train_idx = dataset_into_batch_idx(train_data, BATCH_SIZE)\n",
        "batch_idx = train_idx[0]\n",
        "batch_source_embedded, batch_target, batch_seq_lengths = prepare_batch(train_data, batch_idx)\n",
        "\n",
        "model.hidden = model.init_hidden()\n",
        "output = model(batch_source_embedded, batch_seq_lengths)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('Input:', torch.Size([10, 30, 200]))\n",
            "('After packing:', torch.Size([155, 200]))\n",
            "('Packed output after LSTM:', torch.Size([155, 600]))\n",
            "('Top hidden layer from LSTM:', torch.Size([10, 600]))\n",
            "('Output:', torch.Size([10]))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "edKcx5m8sUCA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Train model"
      ]
    },
    {
      "metadata": {
        "id": "6ikw2Rz4sUCB",
        "colab_type": "code",
        "outputId": "ad4ff20d-4f1d-4d5c-c72d-c5c742f6af90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 94
        }
      },
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "import datetime\n",
        "\n",
        "torch.manual_seed(1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f2c105ae690>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "metadata": {
        "id": "KkHl_auasUCG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_model(target, hidden_dim, batch_size,\n",
        "                epochs, print_every, log_every,\n",
        "                num_layers, dropout, print_summary=True):\n",
        "    \n",
        "    \"\"\"Full training sequence for the model\"\"\"\n",
        "\n",
        "    model = LSTMClassifier(hidden_dim=hidden_dim,\n",
        "                           batch_size=batch_size,\n",
        "                           embedding_size=EMBEDDING_DIM,\n",
        "                           num_layers=num_layers,\n",
        "                           dropout=dropout,\n",
        "                           verbose=False)\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "    losses, validation_losses = [], [] # for plotting losses\n",
        "    offset_count = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(\"\\n=== Epoch {} ===\".format(epoch+1))\n",
        "\n",
        "        # Get shuffled indices for the entire training data set,\n",
        "        # organized into batches:\n",
        "        train_idx = dataset_into_batch_idx(train_data, batch_size)\n",
        "        print(\"{} batches in epoch\".format(train_idx.shape[0]))\n",
        "\n",
        "        for i in range(train_idx.shape[0]):  # Iterate through each batch\n",
        "            batch_idx = train_idx[i]\n",
        "            batch_source_embedded, batch_target, batch_seq_lengths = prepare_batch(train_data, batch_idx, target=target)\n",
        "\n",
        "            model.zero_grad()\n",
        "            model.hidden = model.init_hidden()\n",
        "\n",
        "            output = model(batch_source_embedded, batch_seq_lengths)\n",
        "\n",
        "            loss = criterion(output, batch_target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Log\n",
        "            if i % log_every == 0:\n",
        "                losses.append((i+offset_count, loss)) # for plotting losses\n",
        "\n",
        "            # Validate\n",
        "            if i % print_every == 0:\n",
        "                print('\\nStep {}:'.format(i))\n",
        "                print(datetime.datetime.now().strftime(\"%m/%d/%y %H:%M:%S\"))\n",
        "                print('Training loss: {:.3f}'.format(loss))\n",
        "                with torch.no_grad():\n",
        "                    val_idx = dataset_into_batch_idx(val_data, batch_size, max_batches_to_return=1)\n",
        "                    batch_source_embedded, batch_target, batch_seq_lengths = prepare_batch(train_data, val_idx[0])\n",
        "                    model.hidden = model.init_hidden()\n",
        "                    output = model(batch_source_embedded, batch_seq_lengths)\n",
        "                    validation_loss = criterion(output, batch_target)\n",
        "                    print('Validation loss: {:.3f}'.format(validation_loss))\n",
        "                    validation_losses.append((i+offset_count,validation_loss)) # for plotting losses\n",
        "\n",
        "        offset_count += train_idx.shape[0] # for logging the step count after the first epoch\n",
        "    print(\"=== Done Training ===\")\n",
        "    \n",
        "    if print_summary:\n",
        "        # Print parameters\n",
        "        print(\"\\n=== SUMMARY ==\")\n",
        "        print(\"MODEL PARAMETERS\")\n",
        "        print(\"Target:\", target)\n",
        "        print(\"Hidden dim:\", hidden_dim)\n",
        "        print(\"Number of layers:\", num_layers)\n",
        "        print(\"Dropout:\", dropout)\n",
        "        print(\"\\nTRAINING PARAMETERS\")\n",
        "        print(\"Epochs:\", epochs)\n",
        "        print(\"Batch size:\", batch_size)\n",
        "        \n",
        "       \n",
        "    return model , losses, validation_losses"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8rnI0nsBsUCJ",
        "colab_type": "code",
        "outputId": "2d828ee8-26a9-4d1b-a132-7e757cc37149",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1723
        }
      },
      "cell_type": "code",
      "source": [
        "# Target: is_hate_speech\n",
        "model ,losses, validation_losses = train_model(target='is_hate_speech', hidden_dim=100,\n",
        "                                        batch_size=50, epochs=5,\n",
        "                                        print_every=150, log_every=5,\n",
        "                                        num_layers=4, dropout=0.1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "=== Epoch 1 ===\n",
            "346 batches in epoch\n",
            "\n",
            "Step 0:\n",
            "01/01/19 12:31:58\n",
            "Training loss: 0.678\n",
            "Validation loss: 0.669\n",
            "\n",
            "Step 150:\n",
            "01/01/19 12:32:55\n",
            "Training loss: 0.112\n",
            "Validation loss: 0.114\n",
            "\n",
            "Step 300:\n",
            "01/01/19 12:33:51\n",
            "Training loss: 0.169\n",
            "Validation loss: 0.113\n",
            "\n",
            "=== Epoch 2 ===\n",
            "346 batches in epoch\n",
            "\n",
            "Step 0:\n",
            "01/01/19 12:34:08\n",
            "Training loss: 0.229\n",
            "Validation loss: 0.179\n",
            "\n",
            "Step 150:\n",
            "01/01/19 12:35:04\n",
            "Training loss: 0.226\n",
            "Validation loss: 0.172\n",
            "\n",
            "Step 300:\n",
            "01/01/19 12:36:01\n",
            "Training loss: 0.086\n",
            "Validation loss: 0.108\n",
            "\n",
            "=== Epoch 3 ===\n",
            "346 batches in epoch\n",
            "\n",
            "Step 0:\n",
            "01/01/19 12:36:18\n",
            "Training loss: 0.382\n",
            "Validation loss: 0.198\n",
            "\n",
            "Step 150:\n",
            "01/01/19 12:37:14\n",
            "Training loss: 0.212\n",
            "Validation loss: 0.221\n",
            "\n",
            "Step 300:\n",
            "01/01/19 12:38:10\n",
            "Training loss: 0.159\n",
            "Validation loss: 0.061\n",
            "\n",
            "=== Epoch 4 ===\n",
            "346 batches in epoch\n",
            "\n",
            "Step 0:\n",
            "01/01/19 12:38:27\n",
            "Training loss: 0.109\n",
            "Validation loss: 0.222\n",
            "\n",
            "Step 150:\n",
            "01/01/19 12:39:23\n",
            "Training loss: 0.053\n",
            "Validation loss: 0.255\n",
            "\n",
            "Step 300:\n",
            "01/01/19 12:40:17\n",
            "Training loss: 0.216\n",
            "Validation loss: 0.120\n",
            "\n",
            "=== Epoch 5 ===\n",
            "346 batches in epoch\n",
            "\n",
            "Step 0:\n",
            "01/01/19 12:40:35\n",
            "Training loss: 0.058\n",
            "Validation loss: 0.159\n",
            "\n",
            "Step 150:\n",
            "01/01/19 12:41:30\n",
            "Training loss: 0.246\n",
            "Validation loss: 0.101\n",
            "\n",
            "Step 300:\n",
            "01/01/19 12:42:26\n",
            "Training loss: 0.142\n",
            "Validation loss: 0.277\n",
            "=== Done Training ===\n",
            "\n",
            "=== SUMMARY ==\n",
            "MODEL PARAMETERS\n",
            "('Target:', 'is_hate_speech')\n",
            "('Hidden dim:', 100)\n",
            "('Number of layers:', 4)\n",
            "('Dropout:', 0.1)\n",
            "\n",
            "TRAINING PARAMETERS\n",
            "('Epochs:', 5)\n",
            "('Batch size:', 50)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XK1ndVkku_U5",
        "colab_type": "code",
        "outputId": "904b5485-9319-4bd8-ac52-9179ddfd7c91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "cell_type": "code",
      "source": [
        "def predict(sentence,embeddings_dict,model, max_seq_length = MAX_LENGTH ):\n",
        "  \n",
        "  sentence_lengths = torch.LongTensor([min(len(sentence),max_seq_length)])\n",
        "  sent_embedded=[]\n",
        "  sentence = sentence[:sentence_lengths]\n",
        "  gloigunda = 0\n",
        "  for i,word in enumerate(tk.tokenize(sentence)):\n",
        "    #print(word in embeddings_dict)\n",
        "    if word in embeddings_dict:\n",
        "      word_embedding = embeddings_dict[word]\n",
        "    else:\n",
        "      word_embedding = embeddings_dict[UNK]\n",
        "    sent_embedded.append(torch.FloatTensor(word_embedding))\n",
        "    gloigunda  = i\n",
        "  #print(max_seq_length,len(sentence))\n",
        "  for padding in range(max_seq_length- gloigunda):\n",
        "    sent_embedded.append(torch.FloatTensor(embeddings_dict[PAD]))\n",
        "  \n",
        "     # print(\"sdg\")\n",
        "    #print(word_embedding)\n",
        "    \n",
        "  batch_source_embedded = Variable(torch.stack(sent_embedded))\n",
        "  batch_source_embedded = batch_source_embedded.view(1,batch_source_embedded.shape[0],batch_source_embedded.shape[1])\n",
        "  max_seq_length = torch.FloatTensor([max_seq_length])\n",
        "  origi = batch_source_embedded\n",
        "  orilen =  max_seq_length\n",
        "  for i in range(49):\n",
        "    batch_source_embedded = torch.cat((origi,batch_source_embedded),0)\n",
        "    max_seq_length = torch.cat((orilen,max_seq_length),0)\n",
        "    \n",
        "  print( max_seq_length.size())\n",
        "  print(batch_source_embedded.size(),\"sdg\")\n",
        "  \n",
        "  return np.average(model(batch_source_embedded ,max_seq_length).detach().numpy(),axis=0)\n",
        "\n",
        "print(predict(\"!!!!Blacks should be raped dead and killed\",embeddings_dict,model,36))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([50])\n",
            "(torch.Size([50, 37, 200]), 'sdg')\n",
            "0.2082946\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4rpby38GjwSs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "checkpoint= {\n",
        "    \"state_dict\" : model.state_dict() ,\n",
        "    \"MAX_LENGTH\" : MAX_LENGTH ,\n",
        "    \"embeddings_dict\" : embeddings_dict\n",
        "}\n",
        "torch.save(checkpoint , \"Hateme.pth\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}